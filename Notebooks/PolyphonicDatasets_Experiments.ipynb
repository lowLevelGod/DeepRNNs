{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11972579,"sourceType":"datasetVersion","datasetId":7509055}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Nottingham Dataset","metadata":{}},{"cell_type":"code","source":"from scipy.io import loadmat\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\nclass PianoRollDataset(Dataset):\n    def __init__(self, sequences, sequence_length=100):\n        self.inputs, self.targets = [], []\n        for seq in sequences:\n            if seq.shape[0] > sequence_length:\n                for i in range(seq.shape[0] - sequence_length):\n                    self.inputs.append(seq[i:i+sequence_length])\n                    self.targets.append(seq[i+1:i+sequence_length+1])\n        self.inputs = torch.stack(self.inputs, dim=0)\n        self.targets = torch.stack(self.targets, dim=0)\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        return self.inputs[idx], self.targets[idx]\n\ndef get_nottingham_dataloaders(data_path, batch_size=64):\n    def data_generator(path):\n        data = loadmat(path)\n        X_train = data['traindata'][0]\n        X_valid = data['validdata'][0]\n        X_test = data['testdata'][0]\n        \n        for data in [X_train, X_valid, X_test]:\n            for i in range(len(data)):\n                data[i] = torch.Tensor(data[i].astype(np.float64))\n        \n        return X_train, X_valid, X_test\n    X_train_nottingham, X_val_nottingham, X_test_nottingham = data_generator(data_path)\n    # Dataset and DataLoader\n    train_dataset = PianoRollDataset(X_train_nottingham)\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    \n    val_dataset = PianoRollDataset(X_val_nottingham)\n    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n    \n    test_dataset = PianoRollDataset(X_test_nottingham)\n    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\n    return train_dataloader, val_dataloader, test_dataloader\n\nnottingham_train_dataloader, nottingham_val_dataloader, nottingham_test_dataloader = get_nottingham_dataloaders(\n    \"/kaggle/input/nottingham-music/Nottingham.mat\",\n    batch_size=64)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-03T08:50:47.523106Z","iopub.execute_input":"2025-06-03T08:50:47.523346Z","iopub.status.idle":"2025-06-03T08:51:12.597924Z","shell.execute_reply.started":"2025-06-03T08:50:47.523326Z","shell.execute_reply":"2025-06-03T08:51:12.596857Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Muse Dataset","metadata":{}},{"cell_type":"code","source":"!pip install music21","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T08:51:12.600331Z","iopub.execute_input":"2025-06-03T08:51:12.600794Z","iopub.status.idle":"2025-06-03T08:51:18.250388Z","shell.execute_reply.started":"2025-06-03T08:51:12.600768Z","shell.execute_reply":"2025-06-03T08:51:18.249138Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: music21 in /usr/local/lib/python3.11/dist-packages (9.3.0)\nRequirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from music21) (5.2.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from music21) (1.5.0)\nRequirement already satisfied: jsonpickle in /usr/local/lib/python3.11/dist-packages (from music21) (4.0.5)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from music21) (3.7.2)\nRequirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from music21) (10.6.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from music21) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from music21) (2.32.3)\nRequirement already satisfied: webcolors>=1.5 in /usr/local/lib/python3.11/dist-packages (from music21) (24.11.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->music21) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->music21) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->music21) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->music21) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->music21) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->music21) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->music21) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->music21) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->music21) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->music21) (2025.4.26)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->music21) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->music21) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->music21) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->music21) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->music21) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->music21) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom music21 import corpus, note, chord\nimport random\n\n# Convert a music21 score to a binary piano roll matrix\ndef score_to_pianoroll(score, time_step=0.25, pitch_range=(21, 109), max_length=500):\n    lower, upper = pitch_range\n    pr = np.zeros((max_length, upper - lower), dtype=np.float32)\n    for n in score.flat.notes:\n        if isinstance(n, note.Note):\n            pitches = [n.pitch.midi]\n        elif isinstance(n, chord.Chord):\n            pitches = [p.midi for p in n.pitches]\n        else:\n            continue\n        onset = int(n.offset / time_step)\n        duration = int(n.quarterLength / time_step)\n        if onset < max_length:\n            for p in pitches:\n                if lower <= p < upper:\n                    pr[onset : min(onset + duration, max_length), p - lower] = 1.0\n    return pr\n\nclass MuseDataset(Dataset):\n    def __init__(self, sequences, sequence_length=100):\n        self.inputs, self.targets = [], []\n        for seq in sequences:\n            T, _ = seq.shape\n            if T > sequence_length:\n                for i in range(T - sequence_length):\n                    self.inputs.append(seq[i : i + sequence_length])\n                    self.targets.append(seq[i + 1 : i + sequence_length + 1])\n        if len(self.inputs) == 0:\n            raise ValueError(f\"No sequences â‰¥ {sequence_length+1} timesteps found.\")\n        self.inputs = torch.tensor(self.inputs)\n        self.targets = torch.tensor(self.targets)\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        return self.inputs[idx], self.targets[idx]\n\ndef load_musedata_pianorolls(sequence_length=100, seed=42):\n    random.seed(seed)\n    composers = ['beethoven', 'corelli', 'haydn', 'handel', 'scarlatti']\n    all_pieces = []\n    for comp in composers:\n        all_pieces += corpus.getComposer(comp)\n    random.shuffle(all_pieces)\n\n    rolls = []\n    for p in all_pieces:\n        try:\n            score = corpus.parse(p)\n            pr = score_to_pianoroll(score)\n            if pr.shape[0] > sequence_length:\n                rolls.append(pr)\n        except Exception as e:\n            print(f\"Skipping {p} (parse error): {e}\")\n\n    if len(rolls) == 0:\n        raise RuntimeError(\"No valid piano rolls extracted. Try lowering sequence_length.\")\n\n    return rolls\n\ndef get_musedata_dataloaders(batch_size=64, sequence_length=100):\n    rolls = load_musedata_pianorolls(sequence_length=sequence_length)\n    n = len(rolls)\n    train, val, test = rolls[: int(0.8*n)], rolls[int(0.8*n): int(0.9*n)], rolls[int(0.9*n):]\n\n    train_ds = MuseDataset(train, sequence_length)\n    val_ds   = MuseDataset(val,   sequence_length)\n    test_ds  = MuseDataset(test,  sequence_length)\n\n    return (\n        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n        DataLoader(val_ds,   batch_size=batch_size, shuffle=False),\n        DataLoader(test_ds,  batch_size=batch_size, shuffle=False),\n    )\n\nmuse_train_dataloader, muse_val_dataloader, muse_test_dataloader = get_musedata_dataloaders(\n    batch_size=32,\n    sequence_length=100\n)\nprint(\"Train batches:\", len(muse_train_dataloader))\nx, y = next(iter(muse_train_dataloader))\nprint(\"Example batch shapes:\", x.shape, y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T08:51:18.251839Z","iopub.execute_input":"2025-06-03T08:51:18.252290Z","iopub.status.idle":"2025-06-03T08:54:44.923109Z","shell.execute_reply.started":"2025-06-03T08:51:18.252246Z","shell.execute_reply":"2025-06-03T08:54:44.921506Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/music21/musicxml/xmlToM21.py:2206: MusicXMLWarning: Warning: measure 96 in part Violin Iis overfull: 3083/672 > 4.5,assuming 4.5 is correct.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/music21/stream/base.py:3689: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n  return self.iter().getElementsByClass(classFilterList)\n/tmp/ipykernel_35/3075134642.py:37: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n  self.inputs = torch.tensor(self.inputs)\n","output_type":"stream"},{"name":"stdout","text":"Train batches: 363\nExample batch shapes: torch.Size([32, 100, 88]) torch.Size([32, 100, 88])\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# JSB Chorales Dataset","metadata":{}},{"cell_type":"code","source":"import pickle\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# â”€â”€â”€ 1. Load JSB Chorales Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nwith open(\"/kaggle/input/nottingham-music/jsb-chorales-16th.pkl\", \"rb\") as f:\n    jsb_data = pickle.load(f, encoding=\"latin1\")\n# jsb_data is a dict with keys: 'train', 'valid', 'test'\n# Each is a list of sequences, each sequence is a list of time steps,\n# each time step is a list of active MIDI pitches (integers)\n\n# â”€â”€â”€ 2. Preprocess: Convert to piano roll (binary vector per time step) â”€â”€â”€â”€\nall_pitches = set()\nfor split in ['train', 'valid', 'test']:\n    for seq in jsb_data[split]:\n        for chord in seq:\n            all_pitches.update(chord)\nall_pitches = sorted(all_pitches)\npitch2idx = {p: i for i, p in enumerate(all_pitches)}\nnum_pitches = len(all_pitches)\n\ndef seq_to_pianoroll(seq):\n    # seq: list of time steps, each is a list of pitches\n    roll = np.zeros((len(seq), num_pitches), dtype=np.float32)\n    for t, chord in enumerate(seq):\n        for p in chord:\n            roll[t, pitch2idx[p]] = 1.0\n    return roll\n\ndef make_dataset(split, seq_len=32):\n    X, Y = [], []\n    for seq in jsb_data[split]:\n        roll = seq_to_pianoroll(seq)\n        if len(roll) > seq_len:\n            for i in range(len(roll) - seq_len):\n                X.append(roll[i:i+seq_len])\n                Y.append(roll[i+1:i+seq_len+1])\n    return np.stack(X), np.stack(Y)\n\nclass ChoraleDataset(Dataset):\n    def __init__(self, split, seq_len=32):\n        self.X, self.Y = make_dataset(split, seq_len)\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self, idx):\n        return torch.tensor(self.X[idx]), torch.tensor(self.Y[idx])\n\n# â”€â”€â”€ 3. DataLoaders â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nseq_len = 32\nbatch_size = 64\ntrain_ds = ChoraleDataset('train', seq_len)\nval_ds   = ChoraleDataset('valid', seq_len)\ntest_ds  = ChoraleDataset('test', seq_len)\njsb_train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\njsb_val_dataloader   = DataLoader(val_ds, batch_size=batch_size)\njsb_test_dataloader  = DataLoader(test_ds, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T08:54:44.924465Z","iopub.execute_input":"2025-06-03T08:54:44.924928Z","iopub.status.idle":"2025-06-03T08:54:45.853483Z","shell.execute_reply.started":"2025-06-03T08:54:44.924880Z","shell.execute_reply":"2025-06-03T08:54:45.852488Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Shallow RNN","metadata":{}},{"cell_type":"code","source":"class ShallowRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, rnn_type='RNN'):\n        super().__init__()\n        rnn_cls = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[rnn_type]\n        self.rnn_type = rnn_type\n        self.rnn = rnn_cls(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        output, _ = self.rnn(x)\n        return self.fc(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T08:54:45.854556Z","iopub.execute_input":"2025-06-03T08:54:45.854892Z","iopub.status.idle":"2025-06-03T08:54:45.861910Z","shell.execute_reply.started":"2025-06-03T08:54:45.854867Z","shell.execute_reply":"2025-06-03T08:54:45.860829Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# DT(S)-RNN","metadata":{}},{"cell_type":"code","source":"class DeepTransitionRNNCell(nn.Module):\n    def __init__(self, input_size, hidden_size, transition_size, depth, nonlinearity):\n        super().__init__()\n        self.input_layer = nn.Linear(input_size, hidden_size)\n        self.hidden_layers = nn.ModuleList()\n        self.hidden_layers.append(nn.Linear(hidden_size, transition_size))\n        for i in range(depth - 2):\n            self.hidden_layers.append(nn.Linear(transition_size, transition_size))\n        self.hidden_layers.append(nn.Linear(transition_size, hidden_size))\n\n        if nonlinearity == 'sigmoid':\n            self.activation = nn.Sigmoid()\n        else:\n            self.activation = nn.ReLU()\n\n    def forward(self, x, h_prev):\n        h = self.activation(self.input_layer(x) + h_prev)  # shortcut\n        for layer in self.hidden_layers:\n            h = self.activation(layer(h))\n        return h\n\nclass DTRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, transition_size, depth, nonlinearity='sigmoid'):\n        super().__init__()\n        self.cell = DeepTransitionRNNCell(input_size, hidden_size, transition_size, depth, nonlinearity)\n        self.output = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        batch_size, seq_len, _ = x.size()\n        h = torch.zeros(batch_size, self.cell.input_layer.out_features, device=x.device)\n        outputs = []\n        for t in range(seq_len):\n            h = self.cell(x[:, t, :], h)\n            outputs.append(self.output(h))\n        return torch.stack(outputs, dim=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T08:54:45.863282Z","iopub.execute_input":"2025-06-03T08:54:45.863882Z","iopub.status.idle":"2025-06-03T08:54:45.893607Z","shell.execute_reply.started":"2025-06-03T08:54:45.863844Z","shell.execute_reply":"2025-06-03T08:54:45.892322Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# DOT(S)-RNN","metadata":{}},{"cell_type":"code","source":"class DOTSRNN(nn.Module):\n    def __init__(\n        self, \n        input_size, \n        hidden_size, \n        output_size, \n        transition_size, \n        depth, \n        intermediate_output_size, \n        output_depth=2, \n        nonlinearity='sigmoid', \n        intermediate_output_nonlinearity='sigmoid'):\n        super().__init__()\n        self.cell = DeepTransitionRNNCell(input_size, hidden_size, transition_size, depth, nonlinearity)\n        self.output_layers = nn.ModuleList()\n        self.output_layers.append(nn.Linear(hidden_size, intermediate_output_size))\n        for i in range(output_depth - 2):\n            self.output_layers.append(nn.Linear(intermediate_output_size, intermediate_output_size))\n        self.output_layers.append(nn.Linear(intermediate_output_size, hidden_size))\n        self.output_layers.append(nn.Linear(hidden_size, output_size))\n        \n        if intermediate_output_nonlinearity == 'sigmoid':\n            self.activation = nn.Sigmoid()\n        else:\n            self.activation = nn.ReLU()\n\n    def forward(self, x):\n        batch_size, seq_len, _ = x.size()\n        h = torch.zeros(batch_size, self.cell.input_layer.out_features, device=x.device)\n        outputs = []\n        for t in range(seq_len):\n            h = self.cell(x[:, t], h)\n            out = h\n            for layer in self.output_layers:\n                out = self.activation(layer(out))\n            outputs.append(out)\n        return torch.stack(outputs, dim=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T08:54:45.896415Z","iopub.execute_input":"2025-06-03T08:54:45.896730Z","iopub.status.idle":"2025-06-03T08:54:45.916994Z","shell.execute_reply.started":"2025-06-03T08:54:45.896705Z","shell.execute_reply":"2025-06-03T08:54:45.915884Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# sRNN","metadata":{}},{"cell_type":"code","source":"class StackedRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers=2, rnn_type='RNN'):\n        super().__init__()\n        rnn_cls = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[rnn_type]\n        self.rnn_type = rnn_type\n        self.rnn = rnn_cls(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n        self.output = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        output, _ = self.rnn(x)\n        return self.output(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T08:54:45.917959Z","iopub.execute_input":"2025-06-03T08:54:45.918237Z","iopub.status.idle":"2025-06-03T08:54:45.949400Z","shell.execute_reply.started":"2025-06-03T08:54:45.918215Z","shell.execute_reply":"2025-06-03T08:54:45.948493Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torch import optim\n\ndef lr_schedule(step, initial_lr, beta):\n    return initial_lr * (0.1 ** (step / beta))\n\ndef train_and_eval(\n    model, train_loader, val_loader, vocab_size=None, initial_lr=0.1, beta = 2330, epochs=10, device=None, \n    criterion=None, optimizer_class=None, model_name=\"model\", dataset=\"dataset\"):\n    \n    device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    if criterion is None:\n        raise ValueError(\"You must provide a loss function as 'criterion'.\")\n    if optimizer_class is None:\n        optimizer_class = optim.Adam\n    optimizer = optimizer_class(model.parameters(), lr=initial_lr)\n\n    isPolyphonicDataset = dataset in [\"Nottingham\", \"MuseDataset\", \"JSBDataset\"]\n    \n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n    for ep in range(1, epochs+1):\n        # === Training ===\n        model.train()\n        total_train, correct_train, total_train_tokens = 0, 0, 0\n        train_loop = tqdm(train_loader, desc=f\"Epoch {ep}/{epochs} [Train]\", leave=False)\n        for x, y in train_loop:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            loss = criterion(logits, y)\n            preds = (torch.sigmoid(logits) > 0.5).float()\n            correct_train += (preds == y).float().sum().item()\n            total_train_tokens += y.numel()\n            optimizer.zero_grad()\n            loss.backward()\n\n            if isPolyphonicDataset:\n                # Gradient clipping\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n\n            if isPolyphonicDataset:\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] = lr_schedule(ep, initial_lr, beta)\n            \n            total_train += loss.item()\n            train_loop.set_postfix(loss=loss.item())\n        train_losses.append(total_train / len(train_loader))\n        train_accs.append(correct_train / total_train_tokens)\n\n        # === Validation ===\n        model.eval()\n        total_val, correct_val, total_val_tokens = 0, 0, 0\n        with torch.no_grad():\n            val_loop = tqdm(val_loader, desc=f\"Epoch {ep}/{epochs} [Val]\", leave=False)\n            for x, y in val_loop:\n                x, y = x.to(device), y.to(device)\n                logits = model(x)\n                loss = criterion(logits, y)\n                preds = (torch.sigmoid(logits) > 0.5).float()\n                correct_val += (preds == y).float().sum().item()\n                total_val_tokens += y.numel()\n                total_val += loss.item()\n                val_loop.set_postfix(val_loss=loss.item())\n        val_losses.append(total_val / len(val_loader))\n        val_accs.append(correct_val / total_val_tokens)\n\n        print(f\"Epoch {ep}/{epochs}  \"\n              f\"Train: {train_losses[-1]:.4f} (Acc {train_accs[-1]*100:.2f}%)  \"\n              f\"Val: {val_losses[-1]:.4f} (Acc {val_accs[-1]*100:.2f}%)\")\n\n    # === Plotting ===\n    plt.figure(figsize=(12,5))\n    plt.subplot(1,2,1)\n    plt.plot(train_losses, label='Train')\n    plt.plot(val_losses,   label='Val')\n    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.title(\"Loss\")\n    plt.subplot(1,2,2)\n    plt.plot(train_accs, label='Train Acc')\n    plt.plot(val_accs,   label='Val Acc')\n    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.title(\"Accuracy\")\n    plt.savefig(model_name + \"_\" + dataset + \"_loss_acc.pdf\")\n    plt.show()\n    return model, train_losses[-1], train_accs[-1], val_losses[-1], val_accs[-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T08:59:13.871528Z","iopub.execute_input":"2025-06-03T08:59:13.871928Z","iopub.status.idle":"2025-06-03T08:59:13.893900Z","shell.execute_reply.started":"2025-06-03T08:59:13.871901Z","shell.execute_reply":"2025-06-03T08:59:13.892921Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from itertools import product\n\ndef get_config_combinations(model_type, config_dict):\n    keys = list(config_dict.keys())\n    values = list(config_dict.values())\n    for combo in product(*values):\n        yield dict(zip(keys, combo))\n\nhidden_sizes = [10, 50, 100, 150, 200, 400, 600]\ntransition_sizes = [200]\ndepths = [2]\nintermediate_output_sizes = [200]\noutput_depths = [2]\nnum_layers = [2]\nrnn_types = ['RNN', 'LSTM', 'GRU']\n\nmodel_configs = {\n    \"RNN\": {\n        \"hidden_size\": hidden_sizes,\n        \"rnn_type\": rnn_types\n    },\n    \"DT(S)-RNN\": {\n        \"hidden_size\": hidden_sizes,\n        \"transition_size\": transition_sizes,\n        \"depth\": depths\n    },\n    \"DOT(S)-RNN\": {\n        \"hidden_size\": hidden_sizes,\n        \"transition_size\": transition_sizes,\n        \"intermediate_output_size\": intermediate_output_sizes,\n        \"depth\": depths,\n        \"output_depth\": output_depths\n    }\n    ,\n    \"sRNN\": {\n        \"hidden_size\": hidden_sizes,\n        \"num_layers\": num_layers,\n        \"rnn_type\": rnn_types\n    }\n}\n\nbeta_values = {\n    'Nottingham': 2330,  \n    'MuseDataset': 1475,   \n    'JSBDataset': 100   \n}\n\nimport matplotlib.pyplot as plt\n\ndef grid_search(train_dataloader, val_dataloader, input_size, output_size, dataset, num_epochs=10):\n    for model_type, config_options in model_configs.items():\n        train_losses = []\n        val_losses = []\n        train_accuracies = []\n        val_accuracies = []\n        labels = []\n\n        for config in get_config_combinations(model_type, config_options):\n            print(f\"\\nTraining {model_type} with config: {config}\")\n         \n            if model_type == \"RNN\":\n                model = ShallowRNN(input_size=input_size, hidden_size=config[\"hidden_size\"], rnn_type=config[\"rnn_type\"], output_size=output_size)\n    \n            elif model_type == \"DT(S)-RNN\":\n                model = DTRNN(input_size=input_size,\n                              hidden_size=config[\"hidden_size\"],\n                              transition_size=config[\"transition_size\"],\n                              depth=config[\"depth\"],\n                              output_size=output_size)\n    \n            elif model_type == \"DOT(S)-RNN\":\n                model = DOTSRNN(input_size=input_size,\n                                hidden_size=config[\"hidden_size\"],\n                                transition_size=config[\"transition_size\"],\n                                intermediate_output_size=config[\"intermediate_output_size\"],\n                                depth=config[\"depth\"],\n                                output_depth=config[\"output_depth\"],\n                                output_size=output_size)\n    \n            elif model_type == \"sRNN\":\n                model = StackedRNN(input_size=input_size,\n                                   hidden_size=config[\"hidden_size\"],\n                                   num_layers=config[\"num_layers\"],\n                                   rnn_type=config[\"rnn_type\"],\n                                   output_size=output_size)\n    \n            criterion = nn.BCEWithLogitsLoss()\n            optimizer_class = optim.SGD\n                        \n            model, last_train_loss, last_train_acc, last_val_loss, last_val_acc = train_and_eval(\n                model=model,\n                train_loader=train_dataloader,\n                val_loader=val_dataloader,\n                initial_lr=0.1,\n                beta=beta_values[dataset],\n                epochs=num_epochs,\n                model_name=f\"{model_type} {config}\",\n                dataset=dataset,\n                criterion=criterion,\n                optimizer_class=optimizer_class\n            )\n\n            train_losses.append(last_train_loss)\n            val_losses.append(last_val_loss)\n            train_accuracies.append(last_train_acc)\n            val_accuracies.append(last_val_acc)\n\n\n        x = hidden_sizes\n\n        plt.figure(figsize=(14, 6))\n        plt.subplot(1, 2, 1)\n        plt.plot(x, train_losses, marker='o', label='Train Loss')\n        plt.plot(x, val_losses, marker='o', label='Val Loss')\n\n        plt.xlabel(\"Hidden Size\")\n        plt.ylabel(\"Loss\")\n        \n        plt.title(f\"{model_type} Losses on {dataset}\")\n        plt.legend()\n\n        plt.subplot(1, 2, 2)\n        plt.plot(x, train_accuracies, marker='o', label='Train Accuracy')\n        plt.plot(x, val_accuracies, marker='o', label='Val Accuracy')\n\n        plt.xlabel(\"Hidden Size\")\n        plt.ylabel(\"Accuracy\")\n        \n        plt.title(f\"{model_type} Accuracies on {dataset}\")\n        plt.legend()\n        \n        plt.tight_layout()\n        plt.savefig(model_type + \"_\" + dataset + \"_hidden_sizes_\" + \"comparison.pdf\")\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T08:59:16.262347Z","iopub.execute_input":"2025-06-03T08:59:16.262664Z","iopub.status.idle":"2025-06-03T08:59:16.279395Z","shell.execute_reply.started":"2025-06-03T08:59:16.262634Z","shell.execute_reply":"2025-06-03T08:59:16.278081Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"grid_search(nottingham_train_dataloader, nottingham_val_dataloader, input_size = 88, output_size = 88, dataset = \"Nottingham\", num_epochs=10)\ngrid_search(muse_train_dataloader, muse_val_dataloader, input_size = 88, output_size = 88, dataset = \"MuseDataset\", num_epochs=10)\ngrid_search(jsb_train_dataloader, jsb_val_dataloader, input_size = num_pitches, output_size = num_pitches, dataset = \"JSBDataset\", num_epochs=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}