{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treebank Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('treebank')\n",
    "tagged_sents = treebank.tagged_sents()\n",
    "\n",
    "all_tags = sorted({tag for sent in tagged_sents for (_, tag) in sent})\n",
    "tag2idx  = {tag: i for i, tag in enumerate(all_tags)}\n",
    "vocab_size = len(tag2idx)\n",
    "print(f\"Number of POS tags: {vocab_size}\")\n",
    "\n",
    "def tagged_sents_to_word_idx_seqs(tagged, word2idx=None):\n",
    "    # Returns list of word index sequences and tag index sequences\n",
    "    word_set = set(word for sent in tagged for (word, _) in sent)\n",
    "    if word2idx is None:\n",
    "        word2idx = {w: i for i, w in enumerate(sorted(word_set))}\n",
    "    X, Y = [], []\n",
    "    for sent in tagged:\n",
    "        word_idxs = [word2idx[word] for (word, _) in sent]\n",
    "        tag_idxs  = [tag2idx[tag]   for (_, tag) in sent]\n",
    "        if len(word_idxs) >= 2:\n",
    "            X.append(np.array(word_idxs, dtype=np.int64))\n",
    "            Y.append(np.array(tag_idxs, dtype=np.int64))\n",
    "    return X, Y, word2idx\n",
    "\n",
    "class WordLevelTagSequenceDataset(Dataset):\n",
    "    def __init__(self, word_seqs, tag_seqs, sequence_length=30):\n",
    "        self.inputs, self.targets = [], []\n",
    "        for wseq, tseq in zip(word_seqs, tag_seqs):\n",
    "            L = len(wseq)\n",
    "            if L > sequence_length:\n",
    "                for i in range(L - sequence_length):\n",
    "                    self.inputs.append(wseq[i:i+sequence_length])\n",
    "                    self.targets.append(tseq[i+1:i+sequence_length+1])\n",
    "        if not self.inputs:\n",
    "            raise ValueError(\"No sequences long enough—decrease sequence_length.\")\n",
    "        self.inputs  = torch.tensor(self.inputs, dtype=torch.long)\n",
    "        self.targets = torch.tensor(self.targets, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "def get_wordlevel_dataloaders_treebank(batch_size=64, sequence_length=15, val_split=0.1, test_split=0.1, seed=42):\n",
    "    random.seed(seed)\n",
    "    word_seqs, tag_seqs, word2idx = tagged_sents_to_word_idx_seqs(tagged_sents)\n",
    "    idxs = list(range(len(word_seqs)))\n",
    "    random.shuffle(idxs)\n",
    "    N = len(idxs)\n",
    "    n_test = int(N * test_split)\n",
    "    n_val  = int(N * val_split)\n",
    "    test_idx  = idxs[:n_test]\n",
    "    val_idx   = idxs[n_test : n_test + n_val]\n",
    "    train_idx = idxs[n_test + n_val :]\n",
    "    train_ds = WordLevelTagSequenceDataset([word_seqs[i] for i in train_idx], [tag_seqs[i] for i in train_idx], sequence_length)\n",
    "    val_ds   = WordLevelTagSequenceDataset([word_seqs[i] for i in val_idx],   [tag_seqs[i] for i in val_idx],   sequence_length)\n",
    "    test_ds  = WordLevelTagSequenceDataset([word_seqs[i] for i in test_idx],  [tag_seqs[i] for i in test_idx],  sequence_length)\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(val_ds,   batch_size=batch_size, shuffle=False),\n",
    "        DataLoader(test_ds,  batch_size=batch_size, shuffle=False),\n",
    "        word2idx\n",
    "    )\n",
    "\n",
    "def tagged_sents_to_char_idx_seqs(tagged, char2idx=None):\n",
    "    # Returns list of char index sequences (flattened words) and tag index sequences (per char)\n",
    "    char_set = set(c for sent in tagged for (word, _) in sent for c in word)\n",
    "    if char2idx is None:\n",
    "        char2idx = {c: i for i, c in enumerate(sorted(char_set))}\n",
    "    X, Y = [], []\n",
    "    for sent in tagged:\n",
    "        char_idxs = []\n",
    "        tag_idxs = []\n",
    "        for word, tag in sent:\n",
    "            chars = [char2idx[c] for c in word]\n",
    "            char_idxs.extend(chars)\n",
    "            tag_idxs.extend([tag2idx[tag]] * len(chars))\n",
    "        if len(char_idxs) >= 2:\n",
    "            X.append(np.array(char_idxs, dtype=np.int64))\n",
    "            Y.append(np.array(tag_idxs, dtype=np.int64))\n",
    "    return X, Y, char2idx\n",
    "\n",
    "class CharLevelTagSequenceDataset(Dataset):\n",
    "    def __init__(self, char_seqs, tag_seqs, sequence_length=30):\n",
    "        self.inputs, self.targets = [], []\n",
    "        for cseq, tseq in zip(char_seqs, tag_seqs):\n",
    "            L = len(cseq)\n",
    "            if L > sequence_length:\n",
    "                for i in range(L - sequence_length):\n",
    "                    self.inputs.append(cseq[i:i+sequence_length])\n",
    "                    self.targets.append(tseq[i+1:i+sequence_length+1])\n",
    "        if not self.inputs:\n",
    "            raise ValueError(\"No sequences long enough—decrease sequence_length.\")\n",
    "        self.inputs  = torch.tensor(self.inputs, dtype=torch.long)\n",
    "        self.targets = torch.tensor(self.targets, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "        \n",
    "def tagged_sents_to_char_idx_seqs_treebank(tagged, tag2idx, char2idx=None):\n",
    "    # Returns list of char index sequences (flattened words) and tag index sequences (per char)\n",
    "    char_set = set(c for sent in tagged for (word, _) in sent for c in word)\n",
    "    if char2idx is None:\n",
    "        char2idx = {c: i for i, c in enumerate(sorted(char_set))}\n",
    "    X, Y = [], []\n",
    "    for sent in tagged:\n",
    "        char_idxs = []\n",
    "        tag_idxs = []\n",
    "        for word, tag in sent:\n",
    "            chars = [char2idx[c] for c in word]\n",
    "            char_idxs.extend(chars)\n",
    "            tag_idxs.extend([tag2idx[tag]] * len(chars))\n",
    "        if len(char_idxs) >= 2:\n",
    "            X.append(np.array(char_idxs, dtype=np.int64))\n",
    "            Y.append(np.array(tag_idxs, dtype=np.int64))\n",
    "    return X, Y, char2idx\n",
    "\n",
    "def get_charlevel_dataloaders_treebank(batch_size=64, sequence_length=15, val_split=0.1, test_split=0.1, seed=42):\n",
    "    random.seed(seed)\n",
    "    char_seqs, tag_seqs, char2idx = tagged_sents_to_char_idx_seqs_treebank(tagged_sents, tag2idx)\n",
    "    idxs = list(range(len(char_seqs)))\n",
    "    random.shuffle(idxs)\n",
    "    N = len(idxs)\n",
    "    n_test = int(N * test_split)\n",
    "    n_val  = int(N * val_split)\n",
    "    test_idx  = idxs[:n_test]\n",
    "    val_idx   = idxs[n_test : n_test + n_val]\n",
    "    train_idx = idxs[n_test + n_val :]\n",
    "    train_ds = CharLevelTagSequenceDataset([char_seqs[i] for i in train_idx], [tag_seqs[i] for i in train_idx], sequence_length)\n",
    "    val_ds   = CharLevelTagSequenceDataset([char_seqs[i] for i in val_idx],   [tag_seqs[i] for i in val_idx],   sequence_length)\n",
    "    test_ds  = CharLevelTagSequenceDataset([char_seqs[i] for i in test_idx],  [tag_seqs[i] for i in test_idx],  sequence_length)\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(val_ds,   batch_size=batch_size, shuffle=False),\n",
    "        DataLoader(test_ds,  batch_size=batch_size, shuffle=False),\n",
    "        char2idx\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional datasets: conll + brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown, conll2000\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('conll2000')\n",
    "\n",
    "# --- 1. Load tagged sentences for each corpus ---\n",
    "tagged_sents_brown = brown.tagged_sents(tagset='universal')\n",
    "tagged_sents_conll = conll2000.tagged_sents(tagset='universal')\n",
    "\n",
    "# --- 2. Build tag2idx for each corpus ---\n",
    "def build_tag2idx(tagged_sents):\n",
    "    all_tags = sorted({tag for sent in tagged_sents for (_, tag) in sent})\n",
    "    return {tag: i for i, tag in enumerate(all_tags)}\n",
    "\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "tag2idx_brown = build_tag2idx(tagged_sents_brown)\n",
    "tag2idx_conll = build_tag2idx(tagged_sents_conll)\n",
    "\n",
    "# --- 3. Word-level dataloader for any corpus ---\n",
    "def tagged_sents_to_word_idx_seqs_additional(tagged, tag2idx, word2idx=None):\n",
    "    word_set = set(word for sent in tagged for (word, _) in sent)\n",
    "    if word2idx is None:\n",
    "        word2idx = {w: i for i, w in enumerate(sorted(word_set))}\n",
    "    X, Y = [], []\n",
    "    for sent in tagged:\n",
    "        word_idxs = [word2idx[word] for (word, _) in sent]\n",
    "        tag_idxs  = [tag2idx[tag]   for (_, tag) in sent]\n",
    "        if len(word_idxs) >= 2:\n",
    "            X.append(np.array(word_idxs, dtype=np.int64))\n",
    "            Y.append(np.array(tag_idxs, dtype=np.int64))\n",
    "    return X, Y, word2idx\n",
    "\n",
    "def get_wordlevel_dataloaders_from_corpus(tagged_sents, tag2idx, batch_size=64, sequence_length=15, val_split=0.1, test_split=0.1, seed=42):\n",
    "    random.seed(seed)\n",
    "    word_seqs, tag_seqs, word2idx = tagged_sents_to_word_idx_seqs_additional(tagged_sents, tag2idx)\n",
    "    idxs = list(range(len(word_seqs)))\n",
    "    random.shuffle(idxs)\n",
    "    N = len(idxs)\n",
    "    n_test = int(N * test_split)\n",
    "    n_val  = int(N * val_split)\n",
    "    test_idx  = idxs[:n_test]\n",
    "    val_idx   = idxs[n_test : n_test + n_val]\n",
    "    train_idx = idxs[n_test + n_val :]\n",
    "    train_ds = WordLevelTagSequenceDataset([word_seqs[i] for i in train_idx], [tag_seqs[i] for i in train_idx], sequence_length)\n",
    "    val_ds   = WordLevelTagSequenceDataset([word_seqs[i] for i in val_idx],   [tag_seqs[i] for i in val_idx],   sequence_length)\n",
    "    test_ds  = WordLevelTagSequenceDataset([word_seqs[i] for i in test_idx],  [tag_seqs[i] for i in test_idx],  sequence_length)\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(val_ds,   batch_size=batch_size, shuffle=False),\n",
    "        DataLoader(test_ds,  batch_size=batch_size, shuffle=False),\n",
    "        word2idx\n",
    "    )\n",
    "\n",
    "# --- 4. Char-level dataloader for any corpus ---\n",
    "def tagged_sents_to_char_idx_seqs(tagged, tag2idx, char2idx=None):\n",
    "    char_set = set(c for sent in tagged for (word, _) in sent for c in word)\n",
    "    if char2idx is None:\n",
    "        char2idx = {c: i for i, c in enumerate(sorted(char_set))}\n",
    "    X, Y = [], []\n",
    "    for sent in tagged:\n",
    "        char_idxs = []\n",
    "        tag_idxs = []\n",
    "        for word, tag in sent:\n",
    "            chars = [char2idx[c] for c in word]\n",
    "            char_idxs.extend(chars)\n",
    "            tag_idxs.extend([tag2idx[tag]] * len(chars))\n",
    "        if len(char_idxs) >= 2:\n",
    "            X.append(np.array(char_idxs, dtype=np.int64))\n",
    "            Y.append(np.array(tag_idxs, dtype=np.int64))\n",
    "    return X, Y, char2idx\n",
    "\n",
    "def tagged_sents_to_char_idx_seqs(tagged, tag2idx, char2idx=None):\n",
    "    # Returns list of char index sequences (flattened words) and tag index sequences (per char)\n",
    "    char_set = set(c for sent in tagged for (word, _) in sent for c in word)\n",
    "    if char2idx is None:\n",
    "        char2idx = {c: i for i, c in enumerate(sorted(char_set))}\n",
    "    X, Y = [], []\n",
    "    for sent in tagged:\n",
    "        char_idxs = []\n",
    "        tag_idxs = []\n",
    "        for word, tag in sent:\n",
    "            chars = [char2idx[c] for c in word]\n",
    "            char_idxs.extend(chars)\n",
    "            tag_idxs.extend([tag2idx[tag]] * len(chars))\n",
    "        if len(char_idxs) >= 2:\n",
    "            X.append(np.array(char_idxs, dtype=np.int64))\n",
    "            Y.append(np.array(tag_idxs, dtype=np.int64))\n",
    "    return X, Y, char2idx\n",
    "\n",
    "def get_charlevel_dataloaders_from_corpus(tagged_sents, tag2idx, batch_size=64, sequence_length=15, val_split=0.1, test_split=0.1, seed=42):\n",
    "    random.seed(seed)\n",
    "    char_seqs, tag_seqs, char2idx = tagged_sents_to_char_idx_seqs(tagged_sents, tag2idx)\n",
    "    idxs = list(range(len(char_seqs)))\n",
    "    random.shuffle(idxs)\n",
    "    N = len(idxs)\n",
    "    n_test = int(N * test_split)\n",
    "    n_val  = int(N * val_split)\n",
    "    test_idx  = idxs[:n_test]\n",
    "    val_idx   = idxs[n_test : n_test + n_val]\n",
    "    train_idx = idxs[n_test + n_val :]\n",
    "    train_ds = CharLevelTagSequenceDataset([char_seqs[i] for i in train_idx], [tag_seqs[i] for i in train_idx], sequence_length)\n",
    "    val_ds   = CharLevelTagSequenceDataset([char_seqs[i] for i in val_idx],   [tag_seqs[i] for i in val_idx],   sequence_length)\n",
    "    test_ds  = CharLevelTagSequenceDataset([char_seqs[i] for i in test_idx],  [tag_seqs[i] for i in test_idx],  sequence_length)\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(val_ds,   batch_size=batch_size, shuffle=False),\n",
    "        DataLoader(test_ds,  batch_size=batch_size, shuffle=False),\n",
    "        char2idx\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ShallowRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT(S)-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepTransitionRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, transition_size, depth, nonlinearity):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(hidden_size, transition_size))\n",
    "        for i in range(depth - 2):\n",
    "            self.hidden_layers.append(nn.Linear(transition_size, transition_size))\n",
    "        self.hidden_layers.append(nn.Linear(transition_size, hidden_size))\n",
    "\n",
    "        if nonlinearity == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        h = self.activation(self.input_layer(x) + h_prev)  # shortcut\n",
    "        for layer in self.hidden_layers:\n",
    "            h = self.activation(layer(h))\n",
    "        return h\n",
    "\n",
    "class DTRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, transition_size, depth, nonlinearity='sigmoid'):\n",
    "        super().__init__()\n",
    "        self.cell = DeepTransitionRNNCell(input_size, hidden_size, transition_size, depth, nonlinearity)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        h = torch.zeros(batch_size, self.cell.input_layer.out_features, device=x.device)\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            h = self.cell(x[:, t, :], h)\n",
    "            outputs.append(self.output(h))\n",
    "        return torch.stack(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOT(S)-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DOTSRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size, \n",
    "        hidden_size, \n",
    "        output_size, \n",
    "        transition_size, \n",
    "        depth, \n",
    "        intermediate_output_size, \n",
    "        output_depth=2, \n",
    "        nonlinearity='sigmoid', \n",
    "        intermediate_output_nonlinearity='sigmoid'):\n",
    "        super().__init__()\n",
    "        self.cell = DeepTransitionRNNCell(input_size, hidden_size, transition_size, depth, nonlinearity)\n",
    "        self.output_layers = nn.ModuleList()\n",
    "        self.output_layers.append(nn.Linear(hidden_size, intermediate_output_size))\n",
    "        for i in range(output_depth - 2):\n",
    "            self.output_layers.append(nn.Linear(intermediate_output_size, intermediate_output_size))\n",
    "        self.output_layers.append(nn.Linear(intermediate_output_size, hidden_size))\n",
    "        self.output_layers.append(nn.Linear(hidden_size, output_size))\n",
    "        \n",
    "        if intermediate_output_nonlinearity == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        h = torch.zeros(batch_size, self.cell.input_layer.out_features, device=x.device)\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            h = self.cell(x[:, t], h)\n",
    "            out = h\n",
    "            for layer in self.output_layers:\n",
    "                out = self.activation(layer(out))\n",
    "            outputs.append(out)\n",
    "        return torch.stack(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        return self.output(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sanitize_filename(s):\n",
    "    # Remove or replace problematic characters for Windows filenames\n",
    "    return re.sub(r\"[^a-zA-Z0-9_\\-\\.]\", \"_\", str(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapted models for treebank dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "\n",
    "def lr_schedule(step, initial_lr, beta):\n",
    "    return initial_lr * (0.1 ** (step / beta))\n",
    "\n",
    "def train_and_eval(\n",
    "    model, train_loader, val_loader, vocab_size=None, initial_lr=0.1, beta = 2330, epochs=10, device=None, \n",
    "    criterion=None, optimizer_class=None, model_name=\"model\", dataset=\"dataset\"):\n",
    "    \n",
    "    device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = model.to(device)\n",
    "    if criterion is None:\n",
    "        raise ValueError(\"You must provide a loss function as 'criterion'.\")\n",
    "    if optimizer_class is None:\n",
    "        optimizer_class = optim.Adam\n",
    "    optimizer = optimizer_class(model.parameters(), lr=initial_lr)\n",
    "\n",
    "    isPolyphonicDataset = dataset in [\"Nottingham\", \"MuseDataset\", \"JSBDataset\"]\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    for ep in range(1, epochs+1):\n",
    "        # === Training ===\n",
    "        model.train()\n",
    "        total_train, correct_train, total_train_tokens = 0, 0, 0\n",
    "        train_loop = tqdm(train_loader, desc=f\"Epoch {ep}/{epochs} [Train]\", leave=False)\n",
    "        for x, y in train_loop:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "                logits = logits.view(-1, logits.size(-1))\n",
    "                y_flat = y.view(-1)\n",
    "                loss = criterion(logits, y_flat)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                correct_train += (preds == y_flat).sum().item()\n",
    "                total_train_tokens += y_flat.numel()\n",
    "            else:\n",
    "                # Assume BCEWithLogitsLoss for multi-label\n",
    "                loss = criterion(logits, y)\n",
    "                preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "                correct_train += (preds == y).float().sum().item()\n",
    "                total_train_tokens += y.numel()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            if isPolyphonicDataset:\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            if isPolyphonicDataset:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_schedule(ep, initial_lr, beta)\n",
    "            \n",
    "            total_train += loss.item()\n",
    "            train_loop.set_postfix(loss=loss.item())\n",
    "        train_losses.append(total_train / len(train_loader))\n",
    "        train_accs.append(correct_train / total_train_tokens)\n",
    "\n",
    "        model.eval()\n",
    "        total_val, correct_val, total_val_tokens = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            val_loop = tqdm(val_loader, desc=f\"Epoch {ep}/{epochs} [Val]\", leave=False)\n",
    "            for x, y in val_loop:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                logits = model(x)\n",
    "                if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "                    logits = logits.view(-1, logits.size(-1))\n",
    "                    y_flat = y.view(-1)\n",
    "                    loss = criterion(logits, y_flat)\n",
    "                    preds = logits.argmax(dim=-1)\n",
    "                    correct_val += (preds == y_flat).sum().item()\n",
    "                    total_val_tokens += y_flat.numel()\n",
    "                else:\n",
    "                    loss = criterion(logits, y)\n",
    "                    preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "                    correct_val += (preds == y).float().sum().item()\n",
    "                    total_val_tokens += y.numel()\n",
    "                total_val += loss.item()\n",
    "                val_loop.set_postfix(val_loss=loss.item())\n",
    "        val_losses.append(total_val / len(val_loader))\n",
    "        val_accs.append(correct_val / total_val_tokens)\n",
    "\n",
    "        print(f\"Epoch {ep}/{epochs}  \"\n",
    "              f\"Train: {train_losses[-1]:.4f} (Acc {train_accs[-1]*100:.2f}%)  \"\n",
    "              f\"Val: {val_losses[-1]:.4f} (Acc {val_accs[-1]*100:.2f}%)\")\n",
    "        \n",
    "    \n",
    "    # === Plotting ===\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(val_losses,   label='Val')\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.title(\"Loss\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_accs, label='Train Acc')\n",
    "    plt.plot(val_accs,   label='Val Acc')\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.title(\"Accuracy\")\n",
    "    plt.savefig(sanitize_filename(model_name + \"_\" + dataset + \"_loss_acc.pdf\"))\n",
    "    plt.show()\n",
    "    return model, train_losses[-1], train_accs[-1], val_losses[-1], val_accs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptedShallowRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)      \n",
    "        out, _ = self.rnn(x)       \n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)       \n",
    "\n",
    "class AdaptedDTRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, depth, transition_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = DTRNN(embedding_dim, hidden_size, vocab_size, depth=depth, transition_size=transition_size)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        return self.rnn(x)\n",
    "\n",
    "class AdaptedDOTSRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, depth=2, output_depth=2, intermediate_output_size=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = DOTSRNN(embedding_dim, hidden_size, vocab_size, depth, output_depth, intermediate_output_size=intermediate_output_size)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        return self.rnn(x)\n",
    "\n",
    "class AdaptedStackedRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = StackedRNN(embedding_dim, hidden_size, vocab_size, num_layers)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        return self.rnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def get_config_combinations(model_type, config_dict):\n",
    "    keys = list(config_dict.keys())\n",
    "    values = list(config_dict.values())\n",
    "    for combo in product(*values):\n",
    "        yield dict(zip(keys, combo))\n",
    "\n",
    "hidden_sizes = [200]\n",
    "transition_sizes = [100, 200, 300, 400, 500, 600]\n",
    "depths = [3]\n",
    "intermediate_output_sizes = [200]\n",
    "output_depths = [2]\n",
    "num_layers = [2]\n",
    "\n",
    "model_configs = {\n",
    "    # \"RNN\": {\n",
    "    #     \"hidden_size\": hidden_sizes\n",
    "    # },\n",
    "    \"DT(S)-RNN\": {\n",
    "        \"hidden_size\": hidden_sizes,\n",
    "        \"transition_size\": transition_sizes,\n",
    "        \"depth\": depths\n",
    "    },\n",
    "    \"DOT(S)-RNN\": {\n",
    "        \"hidden_size\": hidden_sizes,\n",
    "        \"transition_size\": transition_sizes,\n",
    "        \"intermediate_output_size\": intermediate_output_sizes,\n",
    "        \"depth\": depths,\n",
    "        \"output_depth\": output_depths\n",
    "    },\n",
    "    # \"sRNN\": {\n",
    "    #     \"hidden_size\": hidden_sizes,\n",
    "    #     \"num_layers\": num_layers,\n",
    "    # }\n",
    "}\n",
    "\n",
    "beta_values = {\n",
    "    'treebank': 1000,\n",
    "    'conll': 1000,\n",
    "    'brown': 1000\n",
    "}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def grid_search(train_dataloader, val_dataloader, vocab_size, embedding_dim, dataset, num_epochs=10):\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "\n",
    "    for model_type, config_options in model_configs.items():\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        labels = []\n",
    "\n",
    "        for config in get_config_combinations(model_type, config_options):\n",
    "            print(f\"\\nTraining {model_type} with config: {config}\")\n",
    "\n",
    "            # All models use embedding layer, so pass vocab_size and embedding_dim\n",
    "            if model_type == \"RNN\":\n",
    "                model = AdaptedShallowRNN(vocab_size, embedding_dim, config[\"hidden_size\"])\n",
    "            elif model_type == \"DT(S)-RNN\":\n",
    "                model = AdaptedDTRNN(\n",
    "                    vocab_size, embedding_dim,\n",
    "                    config[\"hidden_size\"],\n",
    "                    depth=config[\"depth\"],\n",
    "                    transition_size=config[\"transition_size\"]\n",
    "                )\n",
    "            elif model_type == \"DOT(S)-RNN\":\n",
    "                model = AdaptedDOTSRNN(\n",
    "                    vocab_size, embedding_dim,\n",
    "                    config[\"hidden_size\"],\n",
    "                    depth=config[\"depth\"],\n",
    "                    output_depth=config[\"output_depth\"],\n",
    "                    intermediate_output_size=config[\"intermediate_output_size\"]\n",
    "                )\n",
    "            elif model_type == \"sRNN\":\n",
    "                model = AdaptedStackedRNN(\n",
    "                    vocab_size, embedding_dim,\n",
    "                    config[\"hidden_size\"],\n",
    "                    num_layers=config[\"num_layers\"]\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer_class = optim.SGD\n",
    "\n",
    "            model, last_train_loss, last_train_acc, last_val_loss, last_val_acc = train_and_eval(\n",
    "                model=model,\n",
    "                train_loader=train_dataloader,\n",
    "                val_loader=val_dataloader,\n",
    "                vocab_size=vocab_size,\n",
    "                initial_lr=0.1,\n",
    "                beta=beta_values.get(dataset, 1000),\n",
    "                epochs=num_epochs,\n",
    "                model_name=f\"{model_type}_{dataset}_{config}\",\n",
    "                dataset=dataset,\n",
    "                criterion=criterion,\n",
    "                optimizer_class=optimizer_class\n",
    "            )\n",
    "\n",
    "            train_losses.append(last_train_loss)\n",
    "            val_losses.append(last_val_loss)\n",
    "            train_accuracies.append(last_train_acc)\n",
    "            val_accuracies.append(last_val_acc)\n",
    "\n",
    "        x = transition_sizes\n",
    "\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(x, train_losses, marker='o', label='Train Loss')\n",
    "        plt.plot(x, val_losses, marker='o', label='Val Loss')\n",
    "        plt.xlabel(\"TransitionSize\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"{model_type} Losses on {dataset}\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(x, train_accuracies, marker='o', label='Train Accuracy')\n",
    "        plt.plot(x, val_accuracies, marker='o', label='Val Accuracy')\n",
    "        plt.xlabel(\"TransitionSize\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(f\"{model_type} Accuracies on {dataset}\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(sanitize_filename(f\"{model_type}_{dataset}_depth_comparison.pdf\"))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "hidden_size = 128\n",
    "epochs        = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Running on GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_word, val_loader_word, test_loader_word, word2idx = get_wordlevel_dataloaders_treebank(\n",
    "    batch_size=32\n",
    ")\n",
    "word_vocab_size = len(word2idx)  \n",
    "\n",
    "# train_loader_char, val_loader_char, test_loader, char2idx = get_charlevel_dataloaders_treebank(\n",
    "#     batch_size=32\n",
    "# )\n",
    "# vocab_size = len(char2idx)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader_word_conll, val_loader_word_conll, test_loader_word_conll, word2idx_conll = get_wordlevel_dataloaders_from_corpus(\n",
    "    tagged_sents_conll, tag2idx_conll, batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader_word_brown, val_loader_word_brown, test_loader_word_brown, word2idx_brown = get_wordlevel_dataloaders_from_corpus(\n",
    "    tagged_sents_brown, tag2idx_brown, batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Treebank ---\n",
    "print(\"Grid search: Treebank word-level\")\n",
    "grid_search(\n",
    "    train_loader_word, val_loader_word,\n",
    "    vocab_size=len(word2idx), embedding_dim=embedding_dim,\n",
    "    dataset='treebank_word_level', num_epochs=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"Grid search: Treebank char-level\")\n",
    "# grid_search(\n",
    "#     train_loader_char, val_loader_char,\n",
    "#     vocab_size=len(char2idx), embedding_dim=embedding_dim,\n",
    "#     dataset='treebank', num_epochs=7\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CoNLL2000 ---\n",
    "print(\"Grid search: CoNLL2000 word-level\")\n",
    "grid_search(\n",
    "    train_loader_word_conll, val_loader_word_conll,\n",
    "    vocab_size=len(word2idx_conll), embedding_dim=embedding_dim,\n",
    "    dataset='conll_word_level', num_epochs=3\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Brown ---\n",
    "print(\"Grid search: Brown word-level\")\n",
    "grid_search(\n",
    "    train_loader_word_brown, val_loader_word_brown,\n",
    "    vocab_size=len(word2idx_brown), embedding_dim=embedding_dim,\n",
    "    dataset='brown_word_level', num_epochs=3\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7520893,
     "sourceId": 11960856,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7509055,
     "sourceId": 11972579,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
